{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ddff0d3",
   "metadata": {},
   "source": [
    "## Testing Model V1 on dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3fba62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports OK.\n"
     ]
    }
   ],
   "source": [
    "# === IMPORTS ===\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "import os, sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from models.model_v1 import get_model\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "print(\"Imports OK.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1e254c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST_DIR = F:\\Projects\\collaborative_cnn_team08\\TestDataset\n",
      "MODEL_PATH = ../models/model_v1.pth\n"
     ]
    }
   ],
   "source": [
    "# === Config ===\n",
    "DATA_DIR = 'F:\\Projects\\collaborative_cnn_team08'\n",
    "TEST_DIR = os.path.join(DATA_DIR, 'TestDataset')\n",
    "MODEL_PATH = '../models/model_v1.pth'\n",
    "OUTPUT_JSON = \"../results/test_v1_user2.json\"\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 4\n",
    "IMAGE_SIZE = (224, 224)\n",
    "\n",
    "os.makedirs(\"../results\", exist_ok=True)\n",
    "print('TEST_DIR =', TEST_DIR)\n",
    "print('MODEL_PATH =', MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cee28552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['cat', 'dog']\n",
      "Number of test images: 1000\n"
     ]
    }
   ],
   "source": [
    "# === LOAD TEST DATASET ===\n",
    "if not os.path.isdir(TEST_DIR):\n",
    "    raise RuntimeError(f\"Test directory not found: {TEST_DIR}\")\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "test_ds = datasets.ImageFolder(TEST_DIR, test_transform)\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "print(\"Classes:\", test_ds.classes)\n",
    "print(\"Number of test images:\", len(test_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9a3806a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# === LOAD MODEL ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "num_classes = len(test_ds.classes)\n",
    "model = get_model(num_classes=num_classes, device=device)\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    raise FileNotFoundError(f\"Model not found at: {MODEL_PATH}\")\n",
    "\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e35097e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classification_metrics(y_true, y_pred, average=\"macro\"):\n",
    "    \"\"\"\n",
    "    Compute common classification metrics for classification tasks.\n",
    "\n",
    "    Args:\n",
    "        y_true (list or np.array): Ground truth labels\n",
    "        y_pred (list or np.array): Predicted labels\n",
    "        average (str): Averaging mode for multi-class classification.\n",
    "                       Options: \"macro\", \"micro\", \"weighted\"\n",
    "\n",
    "    Returns:\n",
    "        dict: Accuracy, F1, Precision, Recall, Confusion matrix\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
    "        \"f1\": float(f1_score(y_true, y_pred, average=average, zero_division=0)),\n",
    "        \"precision\": float(precision_score(y_true, y_pred, average=average, zero_division=0)),\n",
    "        \"recall\": float(recall_score(y_true, y_pred, average=average, zero_division=0)),\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        cm = confusion_matrix(y_true, y_pred).tolist()\n",
    "    except Exception:\n",
    "        cm = None\n",
    "\n",
    "    metrics[\"confusion_matrix\"] = cm\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def save_metrics(metrics: dict, path: str):\n",
    "    \"\"\"\n",
    "    Save a dictionary of metrics to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        metrics (dict): metrics dictionary\n",
    "        path (str): output JSON file path\n",
    "    \"\"\"\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98016999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 32/32 [00:20<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Metrics: {'accuracy': 0.765, 'f1': 0.7642339962538287, 'precision': 0.7684892867708744, 'recall': 0.7649999999999999, 'confusion_matrix': [[411, 89], [146, 354]], 'num_test_images': 1000, 'class_names': ['cat', 'dog']}\n",
      "Saved test metrics at ../results/test_v1_user2.json\n"
     ]
    }
   ],
   "source": [
    "# === EVALUATE MODEL ===\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(imgs)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        all_labels.extend(labels.cpu().tolist())\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "\n",
    "# Compute metrics\n",
    "metrics = compute_classification_metrics(all_labels, all_preds)\n",
    "metrics[\"num_test_images\"] = len(test_ds)\n",
    "metrics[\"class_names\"] = test_ds.classes\n",
    "\n",
    "print(\"Test Metrics:\", metrics)\n",
    "\n",
    "save_metrics(metrics, OUTPUT_JSON)\n",
    "print(\"Saved test metrics at\", OUTPUT_JSON)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77837044",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
